{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1\n",
    "# Data cleaning and saving the cleaned data into an Excel table, with images saved into a specified folder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from openpyxl import load_workbook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to process a single file\n",
    "abnormal_speed = 2.35\n",
    "abnormal_distance = 10.39  # Distance outlier, should not travel that far\n",
    "settled_distance = 8.5  # Typical distance after teleportation in the initial experiment design\n",
    "def process_file(file_path):\n",
    "    # Define column names based on the observed structure\n",
    "    column_names = [\"Time\", \"ID\", \"Positionx\", \"Positionz\", \"Positiony\", \"Yaw\", \"Up\", \"Right\", \"Down\", \"Left\"]\n",
    "\n",
    "    # Read the CSV file without a header and assign column names\n",
    "    data = pd.read_csv(file_path, header=None, names=column_names)\n",
    "\n",
    "    # Adjust time\n",
    "    initial_time = data.loc[0, 'Time']\n",
    "    data['Time'] = data['Time'] - initial_time\n",
    "\n",
    "    # Remove parentheses from the Positionx and Positiony columns\n",
    "    data['Positionx'] = data['Positionx'].astype(str).str.replace('(', '').str.replace(')', '')\n",
    "    data['Positiony'] = data['Positiony'].astype(str).str.replace('(', '').str.replace(')', '')\n",
    "\n",
    "    # Ensure that Positiony and Positionx columns are numeric after cleaning\n",
    "    data['Positiony'] = pd.to_numeric(data['Positiony'], errors='coerce')\n",
    "    data['Positionx'] = pd.to_numeric(data['Positionx'], errors='coerce')\n",
    "\n",
    "    # Calculate the distance from the origin (0, 0) for each row and add a new column 'Distance'\n",
    "    data['Distance'] = np.sqrt(data['Positionx']**2 + data['Positiony']**2)\n",
    "\n",
    "    # Create an empty list to store the row indices that need to be flagged\n",
    "    highlight_rows = []\n",
    "\n",
    "    # Iterate through the Positiony column and filter rows that meet the condition\n",
    "    for i in range(len(data) - 1):\n",
    "        if -1.5 < data.loc[i, 'Positiony'] and abs(data.loc[i, 'Distance'] - data.loc[i + 1, 'Distance']) > 5:\n",
    "            highlight_rows.append(i)\n",
    "            highlight_rows.append(i + 1)\n",
    "\n",
    "    # Create a new column to store the labels\n",
    "    data['Label'] = 0\n",
    "\n",
    "    # Assign labels to the data rows\n",
    "    current_label = 1\n",
    "    if highlight_rows:\n",
    "        data.loc[:highlight_rows[0], 'Label'] = current_label\n",
    "        for i in range(1, len(highlight_rows), 2):\n",
    "            current_label += 1\n",
    "            start_index = highlight_rows[i] + 1\n",
    "            if i + 1 < len(highlight_rows):\n",
    "                end_index = highlight_rows[i + 1]\n",
    "                data.loc[start_index:end_index, 'Label'] = current_label\n",
    "            else:\n",
    "                data.loc[start_index:, 'Label'] = current_label\n",
    "\n",
    "    # Replace intermediate zeros in the Label column with the previous label\n",
    "    for i in range(1, len(data)):\n",
    "        if data.loc[i, 'Label'] == 0:\n",
    "            data.loc[i, 'Label'] = data.loc[i + 1, 'Label']\n",
    "\n",
    "    # Remove five rows of data (useless data during teleportation)\n",
    "\n",
    "    # Create an empty list to store the row indices that need to be cleared\n",
    "    clear_rows = []\n",
    "\n",
    "    # Iterate through the Positiony column and filter rows that meet the condition\n",
    "    for i in range(len(data) - 1):\n",
    "        if -1.5 < data.loc[i, 'Positiony'] and abs(data.loc[i, 'Distance'] - data.loc[i + 1, 'Distance']) > 5:\n",
    "            for j in range(max(0, i-4), i+1):\n",
    "                clear_rows.append(j)\n",
    "\n",
    "    # Define columns to keep\n",
    "    columns_to_keep = ['Time', 'ID']\n",
    "\n",
    "    # Clear data but retain Time and ID information\n",
    "    for row_index in clear_rows:\n",
    "        for col in data.columns:\n",
    "            if col not in columns_to_keep:\n",
    "                data.at[row_index, col] = np.nan  # Clear the content\n",
    "\n",
    "    # Calculate speed (positionx, positiony) at each time unit\n",
    "    # Create a new speed column\n",
    "    data['Speed'] = np.nan\n",
    "\n",
    "    # Iterate through the DataFrame to calculate speed\n",
    "    for i in range(1, len(data)):\n",
    "        if pd.notna(data.at[i, 'Positionx']) and pd.notna(data.at[i, 'Positiony']) and \\\n",
    "           pd.notna(data.at[i-1, 'Positionx']) and pd.notna(data.at[i-1, 'Positiony']):\n",
    "            # Calculate distance\n",
    "            dx = data.at[i, 'Positionx'] - data.at[i-1, 'Positionx']\n",
    "            dy = data.at[i, 'Positiony'] - data.at[i-1, 'Positiony']\n",
    "            distance = np.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            # Calculate time difference\n",
    "            dt = data.at[i, 'Time'] - data.at[i-1, 'Time']\n",
    "\n",
    "            # Calculate speed\n",
    "            if dt != 0:\n",
    "                speed = distance / dt\n",
    "                data.at[i, 'Speed'] = speed\n",
    "    \"\"\"\n",
    "    # Ensure that the first row of each trajectory has a NaN speed value (no inertia)\n",
    "    unique_labels = data['Label'].dropna().unique()\n",
    "    for label in unique_labels:\n",
    "        first_index = data[data['Label'] == label].index.min()\n",
    "        data.at[first_index, 'Speed'] = np.nan\n",
    "    \"\"\"\n",
    "    # Filter rows with speed greater than 2.35 and distance greater than 8.5, and delete all previous rows from these trajectories\n",
    "    filtered_data = data[(data['Speed'] > abnormal_speed) & (data['Distance'] > settled_distance)]\n",
    "\n",
    "    # Find the labels of these rows\n",
    "    labels_to_filter = filtered_data['Label'].unique()\n",
    "\n",
    "    # Iterate through each label that needs to be processed\n",
    "    for label in labels_to_filter:\n",
    "        label_data = data[data['Label'] == label]\n",
    "        # Find the index of the last row to delete\n",
    "        max_index_to_delete = filtered_data[filtered_data['Label'] == label].index.max()\n",
    "        # Retain Time and ID information, clear other columns\n",
    "        for row_index in label_data[label_data.index <= max_index_to_delete].index:\n",
    "            for col in data.columns:\n",
    "                if col not in ['Time', 'ID']:\n",
    "                    data.at[row_index, col] = np.nan  # Set to NaN\n",
    "\n",
    "    # Reset index\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Filter rows with a distance greater than 10.39, and delete all previous rows from these trajectories\n",
    "    \n",
    "    # Find rows that meet the condition\n",
    "    filtered_data = data[data['Distance'] > abnormal_distance]\n",
    "\n",
    "    # Find the labels of these rows\n",
    "    labels_to_filter = filtered_data['Label'].unique()\n",
    "\n",
    "    # Iterate through each label that needs to be processed\n",
    "    for label in labels_to_filter:\n",
    "        label_data = data[data['Label'] == label]\n",
    "        # Find the index of the last row to delete\n",
    "        max_index_to_delete = filtered_data[filtered_data['Label'] == label].index.max()\n",
    "        # Retain Time and ID information, clear other columns\n",
    "        for row_index in label_data[label_data.index <= max_index_to_delete].index:\n",
    "            for col in data.columns:\n",
    "                if col not in ['Time', 'ID']:\n",
    "                    data.at[row_index, col] = np.nan  # Set to NaN\n",
    "\n",
    "    # Reset index\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    \"\"\"\"\n",
    "    # Ensure that the first row of each trajectory has a NaN speed value (no inertia)\n",
    "    unique_labels = data['Label'].dropna().unique()\n",
    "    for label in unique_labels:\n",
    "        first_index = data[data['Label'] == label].index.min()\n",
    "        data.at[first_index, 'Speed'] = np.nan\n",
    "    \"\"\"\n",
    " \n",
    "    # Filter complete trajectories, i.e., delete those trajectories whose distances do not fall within 1.8 to 7\n",
    "    data['Distance'] = pd.to_numeric(data['Distance'], errors='coerce')\n",
    "    # Step 1: Find the minimum and maximum distance for each trajectory\n",
    "    trajectory_stats = data.groupby('Label')['Distance'].agg(['min', 'max']).reset_index()\n",
    "    # Step 2: Identify incomplete trajectories\n",
    "    incomplete_trajectories = trajectory_stats[\n",
    "        ~((trajectory_stats['min'] < 1.8) & (trajectory_stats['max'] > 7))\n",
    "    ]['Label']\n",
    "\n",
    "    # Step 3: Set data to NaN for incomplete trajectories except for ID and Time\n",
    "    for label in incomplete_trajectories:\n",
    "        data.loc[data['Label'] == label, data.columns.difference(['Time', 'ID'])] = np.nan\n",
    "\n",
    "    # Remove stationary points during rest periods (considering congestion cases where movement is impossible)\n",
    "    speed_threshold = 0.1\n",
    "    distance_threshold = 1.8\n",
    "    consecutive_count_threshold = 20\n",
    "\n",
    "    # Used to store labels and indices that meet the condition\n",
    "    labels_to_remove = set()\n",
    "\n",
    "    # Iterate through each label\n",
    "    for label in data['Label'].unique():\n",
    "        if pd.notna(label):\n",
    "            label_data = data[data['Label'] == label]\n",
    "            consecutive_count = 0\n",
    "            for index, row in label_data.iterrows():\n",
    "                if pd.notna(row['Speed']) and row['Speed'] < speed_threshold and row['Distance'] > distance_threshold:\n",
    "                    consecutive_count += 1\n",
    "                    if consecutive_count > consecutive_count_threshold:\n",
    "                        labels_to_remove.add(label)\n",
    "                        break\n",
    "                else:\n",
    "                    consecutive_count = 0\n",
    "\n",
    "    deleted_rows = pd.DataFrame(columns=data.columns)\n",
    "\n",
    "    for label in labels_to_remove:\n",
    "        label_data = data[data['Label'] == label]\n",
    "        max_index_to_delete = label_data.index.max()\n",
    "        deleted_rows = pd.concat([deleted_rows, label_data[label_data.index <= max_index_to_delete]])\n",
    "        for row_index in label_data.index:\n",
    "            if row_index <= max_index_to_delete:\n",
    "                for col in data.columns:\n",
    "                    if col not in ['Time', 'ID']:\n",
    "                        data.at[row_index, col] = np.nan\n",
    "\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Delete idle points before the experiment ends\n",
    "    speed_threshold = 0.1\n",
    "    last_valid_index = data[data['Speed'] >= speed_threshold].index.max()\n",
    "    if not np.isnan(last_valid_index):\n",
    "        for row_index in range(last_valid_index + 1, len(data)):\n",
    "            for col in data.columns:\n",
    "                if col not in ['Time', 'ID']:\n",
    "                    data.at[row_index, col] = np.nan\n",
    "\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    data['AngleChange'] = np.nan\n",
    "\n",
    "    def calculate_angle_change(x0, y0, x1, y1, x2, y2):\n",
    "        v1 = np.array([x1 - x0, y1 - y0])\n",
    "        v2 = np.array([x2 - x1, y2 - y1])\n",
    "        dot_product = np.dot(v1, v2)\n",
    "        norm_v1 = np.linalg.norm(v1)\n",
    "        norm_v2 = np.linalg.norm(v2)\n",
    "        if norm_v1 == 0 or norm_v2 == 0:\n",
    "            return 0\n",
    "        cos_theta = dot_product / (norm_v1 * norm_v2)\n",
    "        cos_theta = np.clip(cos_theta, -1.0, 1.0)\n",
    "        angle = np.arccos(cos_theta) * 180 / np.pi\n",
    "        return angle\n",
    "\n",
    "    for i in range(1, len(data) - 1):\n",
    "        if pd.notna(data.at[i-1, 'Positionx']) and pd.notna(data.at[i-1, 'Positiony']) and \\\n",
    "           pd.notna(data.at[i, 'Positionx']) and pd.notna(data.at[i, 'Positiony']) and \\\n",
    "           pd.notna(data.at[i+1, 'Positionx']) and pd.notna(data.at[i+1, 'Positiony']):\n",
    "            angle_change = calculate_angle_change(\n",
    "                data.at[i-1, 'Positionx'], data.at[i-1, 'Positiony'],\n",
    "                data.at[i, 'Positionx'], data.at[i, 'Positiony'],\n",
    "                data.at[i+1, 'Positionx'], data.at[i+1, 'Positiony']\n",
    "            )\n",
    "            data.at[i, 'AngleChange'] = angle_change\n",
    "\n",
    "    # Ensure that the first row of each trajectory has a NaN speed value (no inertia)\n",
    "    unique_labels = data['Label'].dropna().unique()\n",
    "    for label in unique_labels:\n",
    "        first_index = data[data['Label'] == label].index.min()\n",
    "        data.at[first_index, 'Speed'] = np.nan\n",
    "    \n",
    "    # Calculate SpeedChange\n",
    "    data['SpeedChange'] = np.nan  # Create a new SpeedChange column and initialize with NaN\n",
    "\n",
    "    # Calculate SpeedChange row by row\n",
    "    for i in range(1, len(data)):\n",
    "        if pd.notna(data.at[i, 'Speed']) and pd.notna(data.at[i-1, 'Speed']):\n",
    "          data.at[i, 'SpeedChange'] = data.at[i, 'Speed'] - data.at[i-1, 'Speed']\n",
    "\n",
    "   \n",
    "    return data  # Return the processed data to update start_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_traj(data, output_folder):\n",
    "    # Ensure the output folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    data_with_labels = data.dropna(subset=['Label'])\n",
    "    label_times = data_with_labels.groupby('Label')['Time'].agg(['min', 'max']).reset_index()\n",
    "\n",
    "    for label in label_times['Label'].unique():\n",
    "        track_data = data_with_labels[data_with_labels['Label'] == label]\n",
    "        if track_data.empty:\n",
    "            continue\n",
    "        track_data = track_data.dropna(subset=['Positionx', 'Positiony'])\n",
    "        start_time = label_times[label_times['Label'] == label]['min'].values[0]\n",
    "        end_time = label_times[label_times['Label'] == label]['max'].values[0]\n",
    "        participant_id = track_data['ID'].values[0]\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(track_data['Positionx'], track_data['Positiony'], marker='o')\n",
    "        ax.set_title(f'ID {participant_id} - Track {label} (Start: {start_time}, End: {end_time})')\n",
    "        ax.set_xlabel('Positionx')\n",
    "        ax.set_ylabel('Positiony')\n",
    "        ax.grid(True)\n",
    "        # Save the image to the specified folder\n",
    "        fig.savefig(os.path.join(output_folder, f'track_{participant_id}_label_{label}.png'))\n",
    "        plt.close(fig)  # Close the figure to free up memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder path\n",
    "folder_path = '/Users/yangfanzhou/Desktop/1.8/Experiment 1 data'\n",
    "image_output_folder = '/Users/yangfanzhou/Desktop/1.8/Experiment1_AllTrajectories'\n",
    "\n",
    "csv_file = glob.glob(os.path.join(folder_path, '*.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/9k64w8hn15ng50h2zwfs07tr0000gn/T/ipykernel_48264/2769152573.py:194: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  deleted_rows = pd.concat([deleted_rows, label_data[label_data.index <= max_index_to_delete]])\n",
      "/var/folders/8t/9k64w8hn15ng50h2zwfs07tr0000gn/T/ipykernel_48264/2769152573.py:194: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  deleted_rows = pd.concat([deleted_rows, label_data[label_data.index <= max_index_to_delete]])\n",
      "/var/folders/8t/9k64w8hn15ng50h2zwfs07tr0000gn/T/ipykernel_48264/2769152573.py:194: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  deleted_rows = pd.concat([deleted_rows, label_data[label_data.index <= max_index_to_delete]])\n",
      "/var/folders/8t/9k64w8hn15ng50h2zwfs07tr0000gn/T/ipykernel_48264/2769152573.py:194: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  deleted_rows = pd.concat([deleted_rows, label_data[label_data.index <= max_index_to_delete]])\n",
      "/var/folders/8t/9k64w8hn15ng50h2zwfs07tr0000gn/T/ipykernel_48264/2769152573.py:194: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  deleted_rows = pd.concat([deleted_rows, label_data[label_data.index <= max_index_to_delete]])\n",
      "/var/folders/8t/9k64w8hn15ng50h2zwfs07tr0000gn/T/ipykernel_48264/2769152573.py:194: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  deleted_rows = pd.concat([deleted_rows, label_data[label_data.index <= max_index_to_delete]])\n",
      "/var/folders/8t/9k64w8hn15ng50h2zwfs07tr0000gn/T/ipykernel_48264/2769152573.py:194: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  deleted_rows = pd.concat([deleted_rows, label_data[label_data.index <= max_index_to_delete]])\n",
      "/var/folders/8t/9k64w8hn15ng50h2zwfs07tr0000gn/T/ipykernel_48264/2769152573.py:194: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  deleted_rows = pd.concat([deleted_rows, label_data[label_data.index <= max_index_to_delete]])\n"
     ]
    }
   ],
   "source": [
    "process_dfs = []\n",
    "\n",
    "for file in csv_file:\n",
    "    process_df = process_file(file)\n",
    "    process_dfs.append(process_df)\n",
    "    plot_traj(process_df, image_output_folder)\n",
    "\n",
    "combined_df = pd.concat(process_dfs, ignore_index=True)\n",
    "\n",
    "combined_df\n",
    "\n",
    "# Save combined_df to the specified .xlsx file\n",
    "output_excel_path = '/Users/yangfanzhou/Desktop/1.8/ResultWholeDistance/Experiment1_Data.xlsx'\n",
    "\n",
    "combined_df.to_excel(output_excel_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
